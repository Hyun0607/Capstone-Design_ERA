from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Chroma
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
import pandas as pd
from config import engine, PERSIST_DIR

# âœ… GPT-4 Turbo ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
llm = ChatOpenAI(model_name="gpt-4-turbo", temperature=0.1, max_tokens=256)

template = """
ë‹¹ì‹ ì€ ê³ ë ¹ìë‚˜ ì¥ì• ì¸ì„ ìœ„í•œ ìˆ™ì†Œ ì¶”ì²œ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì‚¬ìš©ì ìš”ì²­ê³¼ ê´€ë ¨ëœ ìˆ™ì†Œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤.
ê° ìˆ™ì†Œëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ, ì™œ ì í•©í•œì§€ 1~2ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
â—ì ˆëŒ€ ìƒˆë¡œìš´ ìˆ™ì†Œëª…ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

ì¶œë ¥ ì˜ˆì‹œ (ì°¸ê³ ìš©):
1. [ìˆ™ì†Œëª…]: íƒ‘ìŠ¤í… ê°•ë¦‰ í˜¸í…”
   [ì„¤ëª…]: íœ ì²´ì–´ ì§„ì… ê°€ëŠ¥, ìš•ì‹¤ ì†ì¡ì´ ìˆì–´ ê³ ë ¹ì ì í•©

2. [ìˆ™ì†Œëª…]: í˜¸í…” ì´ìŠ¤íŠ¸ë‚˜ì¸
   [ì„¤ëª…]: ê³„ë‹¨ ì—†ëŠ” ì¶œì…êµ¬ì™€ ì—˜ë¦¬ë² ì´í„° ìˆì–´ íœ ì²´ì–´ ì´ìš©ì ì í•©

ğŸ‘‡ ì•„ë˜ ìˆ™ì†Œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ìˆ™ì†Œ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}
"""

prompt = PromptTemplate(input_variables=["context", "question"], template=template)
llm_chain = LLMChain(llm=llm, prompt=prompt)

# âœ… CSV â†’ Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
def build_documents(csv_path: str):
    df = pd.read_csv(csv_path)
    df = df.dropna(subset=["ì„¤ëª…ë¬¸", "ìˆ™ì†Œëª…", "ì‹œêµ°êµ¬ëª…"])

    documents = []
    for _, row in df.iterrows():
        ì§ˆë¬¸ = row.get("ì˜ˆìƒ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸", "") or ""
        ì„¤ëª… = row.get("ì„¤ëª…ë¬¸", "")
        if not isinstance(ì„¤ëª…, str) or ì„¤ëª….strip() == "":
            continue

        page_text = f"[ì˜ˆìƒ ì§ˆë¬¸]\n{ì§ˆë¬¸}\n\n[ìˆ™ì†Œ ì„¤ëª…]\n{ì„¤ëª…}"

        documents.append(Document(
            page_content=page_text,
            metadata={
                "ìˆ™ì†Œëª…": str(row["ìˆ™ì†Œëª…"]),
                "ì‹œêµ°êµ¬ëª…": str(row["ì‹œêµ°êµ¬ëª…"])
            }
        ))
    return documents

# âœ… Document â†’ Chroma ë²¡í„°DBì— ì €ì¥
def store_embeddings(documents):
    embedding_model = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=PERSIST_DIR
    )
    vectordb.persist()

#  ì¶”ì²œ í•¨ìˆ˜ (MMR + ë‹¤ì–‘ì„± ì§€ì‹œ ë°˜ì˜)
def recommend_accommodations(query, ì§€ì—­ëª…=None): # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì„ íƒì ì¸ ì§€ì—­ëª…ì„ ì…ë ¥ë°›ì•„ ìˆ™ì†Œë¥¼ ì¶”ì²œí•´ì£¼ëŠ” í•¨ìˆ˜ ì •ì˜
    vectordb = Chroma(
        persist_directory="./chroma_db_v2",
        embedding_function=OpenAIEmbeddings() # ë°”ë¡œìœ„ì— ì„ë² ë”©ì€ ì €ì¥ìš©, ì´ ì„ë² ë”©ì€ ê²€ìƒ‰ìš©ì´ë¼ ë³´ë©´ ë¨
    )

    filter_kwargs = {"ì‹œêµ°êµ¬ëª…": ì§€ì—­ëª…} if ì§€ì—­ëª… else {} # ì§€ì—­ëª…ì„ ì „ë‹¬ ë°›ì•˜ìœ¼ë©´ ì‹œêµ°êµ¬ëª… ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ ì¡°ê±´ ì„¤ì •

    retriever = vectordb.as_retriever( # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ retriever ê°ì²´ë¥¼ ìƒì„±
        search_type="mmr",  # âœ… MMR ë°©ì‹ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
        search_kwargs={"k": 5, "filter": filter_kwargs} # ìµœì¢…ì ìœ¼ë¡œ ìƒìœ„ 5ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰
    )
    docs = retriever.get_relevant_documents(query) # ì‹¤ì œ ì‚¬ìš©ì ì§ˆì˜ ê¸°ë°˜ìœ¼ë¡œ VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ 5ê°œ ê²€ìƒ‰

    context_blocks = [] # LLMì—ê²Œ ì „ë‹¬í•  context(ë¬¸ì„œ í…ìŠ¤íŠ¸)ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° ìœ„í•´ ë¸”ë¡ë³„ë¡œ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ ì–¸
    for i, doc in enumerate(docs, 1): # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© ìˆœíšŒí•˜ë©´ì„œ contextë¥¼ êµ¬ì„± "i"ëŠ” ì¶œë ¥ìš© ë²ˆí˜¸
        ìˆ™ì†Œëª… = doc.metadata.get("ìˆ™ì†Œëª…", "ì´ë¦„ì—†ìŒ") # ë¬¸ì„œì˜ metatdataì—ì„œ "ìˆ™ì†Œëª…"ì„ êº¼ëƒ„
        ì‹œêµ°êµ¬ëª… = doc.metadata.get("ì‹œêµ°êµ¬ëª…", "")
        
        rdb_result = pd.read_sql_query(
            'SELECT * FROM accommodations WHERE ìˆ™ì†Œëª… = %s AND ì‹œêµ°êµ¬ëª… = %s', # RDBì—ì„œ í•´ë‹¹ ìˆ™ì†Œëª…, ì‹œêµ°êµ¬ëª…ì˜ ìƒì„¸ì •ë³´ë¥¼ ì¡°íšŒ
            engine, # SQL ì—°ê²° ê°ì²´, 
            params=(ìˆ™ì†Œëª…, ì‹œêµ°êµ¬ëª…) # ìˆ™ì†Œëª…ì€ íŠœí”Œ í˜•íƒœë¡œ ë„˜ê²¨ì•¼ SQLAlchemyê°€ ì œëŒ€ë¡œ ì¸ì‹í•¨
        )

        if not rdb_result.empty:
            row = rdb_result.iloc[0] # ì¿¼ë¦¬ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì²« ë²ˆì§¸ í–‰ì„ êº¼ëƒ„, í•˜ë‚˜ì˜ ìˆ™ì†Œ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ context ë¸”ë¡ ë§Œë“¬
            context_blocks.append(
                f"{i}. [ìˆ™ì†Œëª…]: {row['ìˆ™ì†Œëª…']}\n"
                f"[ì„¤ëª…]: {doc.page_content}"
            ) # ìˆ™ì†Œëª…ì€ DBì—ì„œ ê°€ì ¸ì˜¨ ê°’, ì„¤ëª…ì€ ë²¡í„°DBì—ì„œ ê°€ì ¸ì˜¨ ê°’
        else:
            context_blocks.append(
                f"{i}. [ìˆ™ì†Œëª…]: {ìˆ™ì†Œëª…}\n"
                f"[ì„¤ëª…]: {doc.page_content}"
            ) # RDBì—ì„œ ìˆ™ì†Œ ìƒì„¸ì •ë³´ê°€ ì—†ì„ ê²½ìš°ì—ë„, metdataì—ì„œ ê°€ì ¸ì˜¨ ìˆ™ì†Œëª…ê³¼ ì„¤ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡ ìƒì„±

    context = "\n\n".join(context_blocks) # ê° ë¸”ë¡ì„ ë‘ ì¤„ ê°„ê²©ìœ¼ë¡œ ì´ì–´ ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ LLM ì…ë ¥ìš© context ì™„ì„±
    if len(context.split()) > 700:
        context = " ".join(context.split()[:700]) # conetxê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° LLM token ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ìë¦„.

    try:
        response = llm_chain.run({"context": context, "question": query})
    except Exception as e:
        response = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"

    return response
