from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Chroma
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
import pandas as pd
from config import engine, PERSIST_DIR

# âœ… GPT-4 Turbo ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
llm = ChatOpenAI(model_name="gpt-4-turbo", temperature=0.1, max_tokens=256)

template = """
ë‹¹ì‹ ì€ ê³ ë ¹ìë‚˜ ì¥ì• ì¸ì„ ìœ„í•œ ìˆ™ì†Œ ì¶”ì²œ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì‚¬ìš©ì ìš”ì²­ê³¼ ê´€ë ¨ëœ ìˆ™ì†Œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤.
ê° ìˆ™ì†Œëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ, ì™œ ì í•©í•œì§€ 1~2ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
â—ì ˆëŒ€ ìƒˆë¡œìš´ ìˆ™ì†Œëª…ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

ì¶œë ¥ ì˜ˆì‹œ (ì°¸ê³ ìš©):
1. [ìˆ™ì†Œëª…]: íƒ‘ìŠ¤í… ê°•ë¦‰ í˜¸í…”
   [ì„¤ëª…]: íœ ì²´ì–´ ì§„ì… ê°€ëŠ¥, ìš•ì‹¤ ì†ì¡ì´ ìˆì–´ ê³ ë ¹ì ì í•©

2. [ìˆ™ì†Œëª…]: í˜¸í…” ì´ìŠ¤íŠ¸ë‚˜ì¸
   [ì„¤ëª…]: ê³„ë‹¨ ì—†ëŠ” ì¶œì…êµ¬ì™€ ì—˜ë¦¬ë² ì´í„° ìˆì–´ íœ ì²´ì–´ ì´ìš©ì ì í•©

ğŸ‘‡ ì•„ë˜ ìˆ™ì†Œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ìˆ™ì†Œ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}
"""

prompt = PromptTemplate(input_variables=["context", "question"], template=template)
llm_chain = LLMChain(llm=llm, prompt=prompt)

# âœ… CSV â†’ Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
def build_documents(csv_path: str):
    df = pd.read_csv(csv_path)
    df = df.dropna(subset=["ì„¤ëª…ë¬¸", "ìˆ™ì†Œëª…", "ì‹œêµ°êµ¬ëª…"])

    documents = []
    for _, row in df.iterrows():
        ì§ˆë¬¸ = row.get("ì˜ˆìƒ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸", "") or ""
        ì„¤ëª… = row.get("ì„¤ëª…ë¬¸", "")
        if not isinstance(ì„¤ëª…, str) or ì„¤ëª….strip() == "":
            continue

        page_text = f"[ì˜ˆìƒ ì§ˆë¬¸]\n{ì§ˆë¬¸}\n\n[ìˆ™ì†Œ ì„¤ëª…]\n{ì„¤ëª…}"

        documents.append(Document(
            page_content=page_text,
            metadata={
                "ìˆ™ì†Œëª…": str(row["ìˆ™ì†Œëª…"]),
                "ì‹œêµ°êµ¬ëª…": str(row["ì‹œêµ°êµ¬ëª…"])
            }
        ))
    return documents

# âœ… Document â†’ Chroma ë²¡í„°DBì— ì €ì¥
def store_embeddings(documents):
    embedding_model = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=PERSIST_DIR
    )
    vectordb.persist()

# âœ… ì‚¬ìš©ì ì¿¼ë¦¬ì— ë”°ë¥¸ ìˆ™ì†Œ ì¶”ì²œ ì‘ë‹µ ìƒì„±
def recommend_accommodations(query, ì§€ì—­ëª…=None):
    vectordb = Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=OpenAIEmbeddings()
    )

    filter_kwargs = {"ì‹œêµ°êµ¬ëª…": ì§€ì—­ëª…} if ì§€ì—­ëª… else {}
    retriever = vectordb.as_retriever(search_kwargs={"k": 5, "filter": filter_kwargs})
    docs = retriever.get_relevant_documents(query)

    context_blocks = []
    for i, doc in enumerate(docs, 1):
        ìˆ™ì†Œëª… = doc.metadata.get("ìˆ™ì†Œëª…", "ì´ë¦„ì—†ìŒ")

        rdb_result = pd.read_sql_query(
            'SELECT * FROM accommodations WHERE ìˆ™ì†Œëª… = %s',
            engine,
            params=(ìˆ™ì†Œëª…,)
        )

        if not rdb_result.empty:
            row = rdb_result.iloc[0]
            context_blocks.append(
                f"{i}. [ìˆ™ì†Œëª…]: {row['ìˆ™ì†Œëª…']}\n[ì„¤ëª…]: {doc.page_content}"
            )
        else:
            context_blocks.append(
                f"{i}. [ìˆ™ì†Œëª…]: {ìˆ™ì†Œëª…}\n[ì„¤ëª…]: {doc.page_content}"
            )

    context = "\n\n".join(context_blocks)
    if len(context.split()) > 700:
        context = " ".join(context.split()[:700])

    try:
        response = llm_chain.run({"context": context, "question": query})
    except Exception as e:
        response = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"

    return response
