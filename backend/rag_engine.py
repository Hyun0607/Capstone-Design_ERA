from vector_db_loader import load_vectordb
from rdb_utils import get_hotels_by_region, build_context_from_sql, get_hotel_info_by_name
from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
import re

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
system_template = """
ë‹¹ì‹ ì€ ê³ ë ¹ìž ë° ìž¥ì• ì¸ì„ ìœ„í•œ ìˆ™ì†Œ ì¶”ì²œ ë„ìš°ë¯¸ìž…ë‹ˆë‹¤.

ì•„ëž˜ëŠ” í›„ë³´ ìˆ™ì†Œ ëª©ë¡ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ì½ê³ , **ê°€ìž¥ ì ì ˆí•œ ìˆ™ì†Œ 3ê³³ì„ ì¶”ì²œ**í•´ ì£¼ì„¸ìš”.
ì¶”ì²œí•  ë•ŒëŠ” ë°˜ë“œì‹œ **contextì— ì£¼ì–´ì§„ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨**í•´ì•¼ í•˜ë©°, **ë‹¤ë¥¸ ì •ë³´ëŠ” ì ˆëŒ€ ì°¸ê³ í•˜ì§€ ë§ˆì„¸ìš”.**
context ì¤‘ì—ì„œë„ **ìˆ™ì†Œ ì„¤ëª…**ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.

[ì¶”ì²œ ê¸°ì¤€]
- ê³ ë ¹ìž, íœ ì²´ì–´ ì´ìš©ìž, ì‹œê°ìž¥ì• ì¸ ë“± **ì´ë™ ë˜ëŠ” ê°ê°ì— ì œì•½ì´ ìžˆëŠ” ì‚¬ìš©ìžì˜ ìš”êµ¬ì‚¬í•­**ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.
- ì§ˆë¬¸ì— ëª…ì‹œëœ ì¡°ê±´(ì˜ˆ: íœ ì²´ì–´, ì—˜ë¦¬ë² ì´í„°, ì ìž ì•ˆë‚´íŒ ë“±)ì— **ì •í™•ížˆ ëŒ€ì‘í•˜ëŠ” ìˆ™ì†Œ**ë¥¼ ì„ ë³„í•˜ì„¸ìš”.
- ë°˜ë“œì‹œ ìˆ™ì†Œ **3ê³³ì„ ì¶”ì²œí•´ì•¼ í•˜ë©°**, 3ê°œ ë¯¸ë§Œì¼ ê²½ìš° ì¶”ì²œëœ ìˆ™ì†Œë§Œ ì¶œë ¥í•˜ì„¸ìš”
- ì¶œë ¥ í˜•ì‹ì€ ì•„ëž˜ ì˜ˆì‹œë¥¼ **ì •í™•ížˆ ê·¸ëŒ€ë¡œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.**

ì¶œë ¥ ì˜ˆì‹œ (ì •í™•ížˆ ì´ í˜•ì‹ì´ì–´ì•¼ í•¨):

1. ìˆ™ì†Œëª…: ì†ì´ˆë°”ë‹¤í˜¸í…”
   ì£¼ì†Œ: í•´ì•ˆë¡œ 406ë²ˆê¸¸ 17
   ìž¥ì• ì¸ íŽ¸ì˜ì‹œì„¤: íœ ì²´ì–´, ì—˜ë¦¬ë² ì´í„°, ì ìž ì•ˆë‚´íŒ

2. ìˆ™ì†Œëª…: ì„¤ì•…ížê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤
   ì£¼ì†Œ: ì¤‘ì•™ë¡œ 233
   ìž¥ì• ì¸ íŽ¸ì˜ì‹œì„¤: íœ ì²´ì–´, ìž¥ì• ì¸ ì£¼ì°¨, ê¸ˆì—° 

3. ìˆ™ì†Œëª…: ìŠ¤ì¹´ì´ë² ì´ ê²½í¬
   ì£¼ì†Œ: ê²½í¬ë¡œ 476
   ìž¥ì• ì¸íŽ¸ì˜ì‹œì„¤: íœ ì²´ì–´, í‚¤ ë‚®ì€ ê°œìˆ˜ëŒ€, ì—˜ë¦¬ë² ì´í„°
"""

# í”„ë¡¬í”„íŠ¸ êµ¬ì„±
system_msg = SystemMessagePromptTemplate.from_template(system_template)
human_msg = HumanMessagePromptTemplate.from_template("{context}\n\n[ì‚¬ìš©ìž ì§ˆë¬¸]\n{question}")
chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])
llm_chain = LLMChain(prompt=chat_prompt, llm=ChatOpenAI(model_name="gpt-4o", temperature=0.7))

# ë²¡í„° DB ë¡œë”©
vectordb = load_vectordb()

def recommend_accommodations(
    user_query: str,
    region_name: str,
    conditions: dict,
    proper_nouns: list,
    top_k: int = 5
) -> list:
    # 1) í”„ë¡ íŠ¸ ìš”ì²­ ë¡œê¹…
    print(f"[DEBUG] [í”„ëŸ°íŠ¸ ìš”ì²­] query={user_query!r}, region={region_name!r}, "
          f"conditions={conditions}, properNouns={proper_nouns}")

    # 2) ì§€ì—­ í•„í„°ë§ í¬í•¨í•œ ë²¡í„° ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° í‚¤ê°€ ì‹¤ì œëŠ” 'region'ì¼ ìˆ˜ë„ ìžˆìœ¼ë‹ˆ í™•ì¸ í•„ìš”)
    try:
        filtered_docs = vectordb.similarity_search(
            user_query,
            k=top_k * 6,
            filter={"ì‹œêµ°êµ¬ëª…": region_name}  # â† ì‹¤ì œ í‚¤ëª…ì´ 'region'ì´ë©´ ì—¬ê¸°ë§Œ ë°”ê¾¸ì„¸ìš”
        )
    except TypeError:
        filtered_docs = vectordb.similarity_search(user_query, k=top_k * 6)

    # 3) RDB í´ë°±: ë²¡í„° í•„í„°ê°€ ë¹„ì—ˆì„ ë•Œë§Œ
    if not filtered_docs:
        region_names = set(get_hotels_by_region(region_name))
        all_docs = vectordb.similarity_search(user_query, k=top_k * 6)
        filtered_docs = [
            doc for doc in all_docs
            if doc.metadata.get("ìˆ™ì†Œëª…") in region_names
        ]

    print(f"[DEBUG] ðŸ“ ì§€ì—­ í•„í„° í›„ ë¬¸ì„œ ìˆ˜: {len(filtered_docs)}")

    # 4) ìƒìœ„ Kê°œ ìˆ™ì†Œ ID ì¶”ì¶œ
    ì¶”ì²œìˆ™ì†Œë“¤ = list({doc.metadata["ìˆ™ì†Œëª…"] for doc in filtered_docs[:top_k]})
    print(f"[DEBUG] âœ… LLM íŒŒì‹± ëŒ€ìƒ ìˆ™ì†Œ: {ì¶”ì²œìˆ™ì†Œë“¤}")

    # 5) RDBì—ì„œ context ìƒì„±
    context = build_context_from_sql(ì¶”ì²œìˆ™ì†Œë“¤)
    print(f"[DEBUG] ðŸ“¦ ìƒì„±ëœ context:\n{context}")

    # 6) ë¹ˆ context ë°©ì–´: ì •ë³´ ì—†ìœ¼ë©´ LLM í˜¸ì¶œ ì—†ì´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
    if not context.strip():
        print("âš ï¸ contextê°€ ë¹„ì–´ ìžˆì–´ LLM í˜¸ì¶œ ì—†ì´ ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []

    # 7) LLM í˜¸ì¶œ (invoke ë©”ì„œë“œë¡œ êµì²´)
    llm_response = llm_chain.invoke({"context": context, "question": user_query})
    # invokeê°€ dictë¥¼ ë°˜í™˜í•˜ë©´ "text" í•„ë“œ ì‚¬ìš©, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ
    raw_output = llm_response["text"] if isinstance(llm_response, dict) and "text" in llm_response else llm_response
    print("[DEBUG] ðŸ¤– LLM ì‘ë‹µ ì›ë¬¸:")
    print(raw_output)

    # 8) LLM ì¶œë ¥ íŒŒì‹± (ë²ˆí˜¸Â·ê³µë°±Â·ì½œë¡  ë³€í˜• ëŒ€ì‘)
    pattern = re.compile(
        r"(?:\d+\.\s*)?ìˆ™ì†Œëª…[:ï¼š]\s*(.+?)\s*[\r\n]+\s*"
        r"ì£¼ì†Œ[:ï¼š]\s*(.+?)\s*[\r\n]+\s*"
        r"ìž¥ì• ì¸\s*íŽ¸ì˜ì‹œì„¤[:ï¼š]\s*(.+)",
        re.IGNORECASE
    )
    parsed = []
    selected_names = []
    for match in pattern.finditer(raw_output):
        name, desc, tags_str = match.groups()
        # ì¤„ë°”ê¿ˆë„ ì‰¼í‘œë¡œ ì¹˜í™˜í•´ ì—¬ëŸ¬ êµ¬ë¶„ìž ëŒ€ì‘
        tags = [t.strip() for t in re.split(r"[,\n;]+", tags_str) if t.strip()]
        parsed.append({
            "name": name.strip(),
            "description": desc.strip(),
            "tags": tags,
            "imageUrl": ""
        })
        selected_names.append(name.strip())

    # 9) í›„ë³´ ìˆ™ì†Œ ì¶”ê°€ (LLM ì¶”ì²œ ì™¸ ë¬¸ì„œ ì¤‘)
    candidates = []
    for doc in filtered_docs:
        name = doc.metadata.get("ìˆ™ì†Œëª…")
        if name and name not in selected_names and name not in [h["name"] for h in candidates]:
            info = get_hotel_info_by_name(name) or {}
            db_tags = info.get("tags", [])
            meta_tags = doc.metadata.get("tags") or []
            final_tags = db_tags if db_tags else meta_tags
            candidates.append({
                "name": info.get("name", name),
                "description": info.get("address", "") or "",
                "tags": final_tags,
                "imageUrl": info.get("imageUrl") or doc.metadata.get("imageUrl", "")
            })
        if len(candidates) >= 3:
            break

    # 10) LLM ê²°ê³¼ + í›„ë³´ ìˆ™ì†Œ ë³‘í•©
    combined = parsed + candidates
    print(f"[DEBUG] âœ… ìµœì¢… ë°˜í™˜ ìˆ™ì†Œ ìˆ˜: {len(combined)}")
    print(f"[DEBUG] âœ… ë°˜í™˜ ë¦¬ìŠ¤íŠ¸ ë‚´ìš©:\n{combined}")

    return combined
